"""
ç®€åŒ–ç‰ˆæ€§èƒ½æµ‹è¯•è„šæœ¬
åªè¿è¡Œä¸€ä¸ª chunk æ¥å¿«é€Ÿå¯¹æ¯”æ€§èƒ½
"""

from pathlib import Path
from time import perf_counter
from dataclasses import dataclass
from typing import List, Optional
import gc

import numpy as np
import torch
from rich.console import Console
from rich.table import Table
from rich.panel import Panel
from rich import box

from sorawm.core import SoraWM
from sorawm.cleaner.e2fgvi_hq_cleaner import E2FGVIHDCleaner, get_ref_index
from sorawm.schemas import CleanerType
from tqdm import tqdm

console = Console()


class StopAfterOneChunk(Exception):
    """ç”¨äºåœ¨ä¸€ä¸ª chunk å®Œæˆååœæ­¢"""
    def __init__(self, elapsed_time: float, result: List[np.ndarray]):
        self.elapsed_time = elapsed_time
        self.result = result
        super().__init__("Benchmark completed for one chunk")


# =============================================================================
# åŸºå‡†æµ‹è¯•ç‰ˆæœ¬çš„ Cleaners
# =============================================================================

class BaselineBenchmarkCleaner(E2FGVIHDCleaner):
    """åŸå§‹ç‰ˆæœ¬ - ç”¨äºåŸºå‡†æµ‹è¯•"""
    
    def process_frames_chunk(
        self,
        chunk_length: int,
        neighbor_stride: int,
        imgs_chunk: torch.Tensor,
        masks_chunk: torch.Tensor,
        binary_masks_chunk: np.ndarray,
        frames_np_chunk: np.ndarray,
        h: int,
        w: int,
    ) -> List[np.ndarray]:
        
        torch.cuda.synchronize()
        start_time = perf_counter()
        
        # åŸå§‹å®ç°
        comp_frames_chunk = [None] * chunk_length
        for f in tqdm(
            range(0, chunk_length, neighbor_stride),
            desc=f"  Frame progress",
            position=1,
            leave=False,
        ):
            neighbor_ids = [
                i
                for i in range(
                    max(0, f - neighbor_stride),
                    min(chunk_length, f + neighbor_stride + 1),
                )
            ]
            ref_ids = get_ref_index(
                f,
                neighbor_ids,
                chunk_length,
                self.config.ref_length,
                self.config.num_ref,
            )
            selected_imgs = imgs_chunk[:1, neighbor_ids + ref_ids, :, :, :]
            selected_masks = masks_chunk[:1, neighbor_ids + ref_ids, :, :, :]
            with torch.no_grad():
                # GPU OPS
                masked_imgs = selected_imgs * (1 - selected_masks)
                mod_size_h = 60
                mod_size_w = 108
                h_pad = (mod_size_h - h % mod_size_h) % mod_size_h
                w_pad = (mod_size_w - w % mod_size_w) % mod_size_w
                masked_imgs = torch.cat([masked_imgs, torch.flip(masked_imgs, [3])], 3)[
                    :, :, :, : h + h_pad, :
                ]
                masked_imgs = torch.cat([masked_imgs, torch.flip(masked_imgs, [4])], 4)[
                    :, :, :, :, : w + w_pad
                ]
                pred_imgs, _ = self.model(masked_imgs, len(neighbor_ids))
                pred_imgs = pred_imgs[:, :, :h, :w]
                pred_imgs = (pred_imgs + 1) / 2
                # CPU OPS
                pred_imgs = pred_imgs.cpu().permute(0, 2, 3, 1).numpy() * 255
                for i in range(len(neighbor_ids)):
                    idx = neighbor_ids[i]
                    img = np.array(pred_imgs[i]).astype(np.uint8) * binary_masks_chunk[
                        idx
                    ] + frames_np_chunk[idx] * (1 - binary_masks_chunk[idx])
                    if comp_frames_chunk[idx] is None:
                        comp_frames_chunk[idx] = img
                    else:
                        comp_frames_chunk[idx] = (
                            comp_frames_chunk[idx].astype(np.float32) * 0.5
                            + img.astype(np.float32) * 0.5
                        )
        
        torch.cuda.synchronize()
        elapsed = perf_counter() - start_time
        
        # æŠ›å‡ºå¼‚å¸¸æ¥åœæ­¢ï¼ŒåŒæ—¶æºå¸¦ç»“æœ
        raise StopAfterOneChunk(elapsed, comp_frames_chunk)


class OptimizedBenchmarkCleaner(E2FGVIHDCleaner):
    """æ–¹æ¡ˆ1: åŒç¼“å†² + CUDA Streams"""
    
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.stream_a = torch.cuda.Stream()
        self.stream_b = torch.cuda.Stream()
    
    def process_frames_chunk(
        self,
        chunk_length: int,
        neighbor_stride: int,
        imgs_chunk: torch.Tensor,
        masks_chunk: torch.Tensor,
        binary_masks_chunk: np.ndarray,
        frames_np_chunk: np.ndarray,
        h: int,
        w: int,
    ) -> List[np.ndarray]:
        
        torch.cuda.synchronize()
        start_time = perf_counter()
        
        comp_frames_chunk = [None] * chunk_length
        
        mod_size_h = 60
        mod_size_w = 108
        h_pad = (mod_size_h - h % mod_size_h) % mod_size_h
        w_pad = (mod_size_w - w % mod_size_w) % mod_size_w
        
        # é¢„è®¡ç®—æ‰¹æ¬¡
        all_batches = []
        for f in range(0, chunk_length, neighbor_stride):
            neighbor_ids = [
                i for i in range(
                    max(0, f - neighbor_stride),
                    min(chunk_length, f + neighbor_stride + 1),
                )
            ]
            ref_ids = get_ref_index(
                f, neighbor_ids, chunk_length,
                self.config.ref_length, self.config.num_ref,
            )
            all_batches.append((neighbor_ids, ref_ids))
        
        # æµæ°´çº¿çŠ¶æ€
        prev_pred_imgs = None
        prev_neighbor_ids = None
        prev_stream = None
        streams = [self.stream_a, self.stream_b]
        
        for batch_idx, (neighbor_ids, ref_ids) in enumerate(
            tqdm(all_batches, desc="  Frame progress", position=1, leave=False)
        ):
            current_stream = streams[batch_idx % 2]
            
            # CPU å¤„ç†ä¸Šä¸€æ‰¹
            if prev_pred_imgs is not None:
                prev_stream.synchronize()
                pred_np = prev_pred_imgs.cpu().permute(0, 2, 3, 1).numpy() * 255
                
                for i in range(len(prev_neighbor_ids)):
                    idx = prev_neighbor_ids[i]
                    img = (
                        np.array(pred_np[i]).astype(np.uint8)
                        * binary_masks_chunk[idx]
                        + frames_np_chunk[idx] * (1 - binary_masks_chunk[idx])
                    )
                    if comp_frames_chunk[idx] is None:
                        comp_frames_chunk[idx] = img
                    else:
                        comp_frames_chunk[idx] = (
                            comp_frames_chunk[idx].astype(np.float32) * 0.5
                            + img.astype(np.float32) * 0.5
                        )
            
            # GPU å¤„ç†å½“å‰æ‰¹
            with torch.cuda.stream(current_stream):
                selected_imgs = imgs_chunk[:1, neighbor_ids + ref_ids, :, :, :]
                selected_masks = masks_chunk[:1, neighbor_ids + ref_ids, :, :, :]
                
                with torch.no_grad():
                    masked_imgs = selected_imgs * (1 - selected_masks)
                    masked_imgs = torch.cat(
                        [masked_imgs, torch.flip(masked_imgs, [3])], 3
                    )[:, :, :, : h + h_pad, :]
                    masked_imgs = torch.cat(
                        [masked_imgs, torch.flip(masked_imgs, [4])], 4
                    )[:, :, :, :, : w + w_pad]
                    
                    pred_imgs, _ = self.model(masked_imgs, len(neighbor_ids))
                    pred_imgs = pred_imgs[:, :, :h, :w]
                    pred_imgs = (pred_imgs + 1) / 2
            
            prev_pred_imgs = pred_imgs
            prev_neighbor_ids = neighbor_ids
            prev_stream = current_stream
        
        # å¤„ç†æœ€åä¸€æ‰¹
        if prev_pred_imgs is not None:
            prev_stream.synchronize()
            pred_np = prev_pred_imgs.cpu().permute(0, 2, 3, 1).numpy() * 255
            
            for i in range(len(prev_neighbor_ids)):
                idx = prev_neighbor_ids[i]
                img = (
                    np.array(pred_np[i]).astype(np.uint8)
                    * binary_masks_chunk[idx]
                    + frames_np_chunk[idx] * (1 - binary_masks_chunk[idx])
                )
                if comp_frames_chunk[idx] is None:
                    comp_frames_chunk[idx] = img
                else:
                    comp_frames_chunk[idx] = (
                        comp_frames_chunk[idx].astype(np.float32) * 0.5
                        + img.astype(np.float32) * 0.5
                    )
        
        torch.cuda.synchronize()
        elapsed = perf_counter() - start_time
        raise StopAfterOneChunk(elapsed, comp_frames_chunk)


class ThreadPoolBenchmarkCleaner(E2FGVIHDCleaner):
    """æ–¹æ¡ˆ2: å¤šçº¿ç¨‹ + Streams"""
    
    def __init__(self, *args, num_cpu_workers: int = 4, **kwargs):
        super().__init__(*args, **kwargs)
        self.num_cpu_workers = num_cpu_workers
        self.stream_a = torch.cuda.Stream()
        self.stream_b = torch.cuda.Stream()
    
    def process_frames_chunk(
        self,
        chunk_length: int,
        neighbor_stride: int,
        imgs_chunk: torch.Tensor,
        masks_chunk: torch.Tensor,
        binary_masks_chunk: np.ndarray,
        frames_np_chunk: np.ndarray,
        h: int,
        w: int,
    ) -> List[np.ndarray]:
        from concurrent.futures import ThreadPoolExecutor
        from threading import Lock
        
        torch.cuda.synchronize()
        start_time = perf_counter()
        
        comp_frames_chunk = [None] * chunk_length
        results_lock = Lock()
        
        mod_size_h = 60
        mod_size_w = 108
        h_pad = (mod_size_h - h % mod_size_h) % mod_size_h
        w_pad = (mod_size_w - w % mod_size_w) % mod_size_w
        
        all_batches = []
        for f in range(0, chunk_length, neighbor_stride):
            neighbor_ids = [
                i for i in range(
                    max(0, f - neighbor_stride),
                    min(chunk_length, f + neighbor_stride + 1),
                )
            ]
            ref_ids = get_ref_index(
                f, neighbor_ids, chunk_length,
                self.config.ref_length, self.config.num_ref,
            )
            all_batches.append((neighbor_ids, ref_ids))
        
        def cpu_postprocess(pred_np: np.ndarray, neighbor_ids: List[int]):
            for i in range(len(neighbor_ids)):
                idx = neighbor_ids[i]
                img = (
                    np.array(pred_np[i]).astype(np.uint8)
                    * binary_masks_chunk[idx]
                    + frames_np_chunk[idx] * (1 - binary_masks_chunk[idx])
                )
                with results_lock:
                    if comp_frames_chunk[idx] is None:
                        comp_frames_chunk[idx] = img
                    else:
                        comp_frames_chunk[idx] = (
                            comp_frames_chunk[idx].astype(np.float32) * 0.5
                            + img.astype(np.float32) * 0.5
                        )
        
        with ThreadPoolExecutor(max_workers=self.num_cpu_workers) as executor:
            futures = []
            streams = [self.stream_a, self.stream_b]
            events = [torch.cuda.Event() for _ in range(len(all_batches))]
            gpu_results = {}
            
            for batch_idx, (neighbor_ids, ref_ids) in enumerate(
                tqdm(all_batches, desc="  Frame progress", position=1, leave=False)
            ):
                stream_idx = batch_idx % 2
                stream = streams[stream_idx]
                
                if batch_idx >= 2:
                    prev_idx = batch_idx - 2
                    events[prev_idx].synchronize()
                    
                    if prev_idx in gpu_results:
                        prev_tensor, prev_neighbors = gpu_results.pop(prev_idx)
                        pred_np = prev_tensor.cpu().permute(0, 2, 3, 1).numpy() * 255
                        future = executor.submit(cpu_postprocess, pred_np, prev_neighbors)
                        futures.append(future)
                
                with torch.cuda.stream(stream):
                    selected_imgs = imgs_chunk[:1, neighbor_ids + ref_ids, :, :, :]
                    selected_masks = masks_chunk[:1, neighbor_ids + ref_ids, :, :, :]
                    
                    with torch.no_grad():
                        masked_imgs = selected_imgs * (1 - selected_masks)
                        masked_imgs = torch.cat(
                            [masked_imgs, torch.flip(masked_imgs, [3])], 3
                        )[:, :, :, : h + h_pad, :]
                        masked_imgs = torch.cat(
                            [masked_imgs, torch.flip(masked_imgs, [4])], 4
                        )[:, :, :, :, : w + w_pad]
                        
                        pred_imgs, _ = self.model(masked_imgs, len(neighbor_ids))
                        pred_imgs = pred_imgs[:, :, :h, :w]
                        pred_imgs = (pred_imgs + 1) / 2
                    
                    events[batch_idx].record(stream)
                
                gpu_results[batch_idx] = (pred_imgs, neighbor_ids)
            
            for remaining_idx in sorted(gpu_results.keys()):
                events[remaining_idx].synchronize()
                tensor, neighbors = gpu_results[remaining_idx]
                pred_np = tensor.cpu().permute(0, 2, 3, 1).numpy() * 255
                future = executor.submit(cpu_postprocess, pred_np, neighbors)
                futures.append(future)
            
            for future in futures:
                future.result()
        
        torch.cuda.synchronize()
        elapsed = perf_counter() - start_time
        raise StopAfterOneChunk(elapsed, comp_frames_chunk)


class PinnedMemoryBenchmarkCleaner(E2FGVIHDCleaner):
    """æ–¹æ¡ˆ3: Pinned Memory + å¼‚æ­¥ä¼ è¾“"""
    
    def __init__(self, *args, max_neighbors: int = 20, **kwargs):
        super().__init__(*args, **kwargs)
        self.max_neighbors = max_neighbors
        self.stream = torch.cuda.Stream()
        self.pinned_buffer_1: Optional[torch.Tensor] = None
        self.pinned_buffer_2: Optional[torch.Tensor] = None
    
    def _ensure_pinned_buffers(self, h: int, w: int):
        if self.pinned_buffer_1 is None:
            self.pinned_buffer_1 = torch.empty(
                (self.max_neighbors, h, w, 3),
                dtype=torch.float32,
                pin_memory=True
            )
            self.pinned_buffer_2 = torch.empty_like(self.pinned_buffer_1)
    
    def process_frames_chunk(
        self,
        chunk_length: int,
        neighbor_stride: int,
        imgs_chunk: torch.Tensor,
        masks_chunk: torch.Tensor,
        binary_masks_chunk: np.ndarray,
        frames_np_chunk: np.ndarray,
        h: int,
        w: int,
    ) -> List[np.ndarray]:
        
        torch.cuda.synchronize()
        start_time = perf_counter()
        
        self._ensure_pinned_buffers(h, w)
        buffers = [self.pinned_buffer_1, self.pinned_buffer_2]
        
        comp_frames_chunk = [None] * chunk_length
        
        mod_size_h = 60
        mod_size_w = 108
        h_pad = (mod_size_h - h % mod_size_h) % mod_size_h
        w_pad = (mod_size_w - w % mod_size_w) % mod_size_w
        
        all_batches = []
        for f in range(0, chunk_length, neighbor_stride):
            neighbor_ids = [
                i for i in range(
                    max(0, f - neighbor_stride),
                    min(chunk_length, f + neighbor_stride + 1),
                )
            ]
            ref_ids = get_ref_index(
                f, neighbor_ids, chunk_length,
                self.config.ref_length, self.config.num_ref,
            )
            all_batches.append((neighbor_ids, ref_ids))
        
        prev_event = None
        prev_buffer = None
        prev_neighbor_ids = None
        prev_count = 0
        
        for batch_idx, (neighbor_ids, ref_ids) in enumerate(
            tqdm(all_batches, desc="  Frame progress", position=1, leave=False)
        ):
            buffer_idx = batch_idx % 2
            current_buffer = buffers[buffer_idx]
            
            # CPU å¤„ç†ä¸Šä¸€æ‰¹
            if prev_event is not None:
                prev_event.synchronize()
                pred_np = prev_buffer[:prev_count].numpy() * 255
                
                for i in range(len(prev_neighbor_ids)):
                    idx = prev_neighbor_ids[i]
                    img = (
                        np.array(pred_np[i]).astype(np.uint8)
                        * binary_masks_chunk[idx]
                        + frames_np_chunk[idx] * (1 - binary_masks_chunk[idx])
                    )
                    if comp_frames_chunk[idx] is None:
                        comp_frames_chunk[idx] = img
                    else:
                        comp_frames_chunk[idx] = (
                            comp_frames_chunk[idx].astype(np.float32) * 0.5
                            + img.astype(np.float32) * 0.5
                        )
            
            # GPU æ“ä½œ
            with torch.cuda.stream(self.stream):
                selected_imgs = imgs_chunk[:1, neighbor_ids + ref_ids, :, :, :]
                selected_masks = masks_chunk[:1, neighbor_ids + ref_ids, :, :, :]
                
                with torch.no_grad():
                    masked_imgs = selected_imgs * (1 - selected_masks)
                    masked_imgs = torch.cat(
                        [masked_imgs, torch.flip(masked_imgs, [3])], 3
                    )[:, :, :, : h + h_pad, :]
                    masked_imgs = torch.cat(
                        [masked_imgs, torch.flip(masked_imgs, [4])], 4
                    )[:, :, :, :, : w + w_pad]
                    
                    pred_imgs, _ = self.model(masked_imgs, len(neighbor_ids))
                    pred_imgs = pred_imgs[:, :, :h, :w]
                    pred_imgs = (pred_imgs + 1) / 2
                    
                    # éé˜»å¡ä¼ è¾“
                    pred_permuted = pred_imgs.permute(0, 2, 3, 1)
                    current_buffer[:len(neighbor_ids)].copy_(pred_permuted, non_blocking=True)
                    
                    current_event = torch.cuda.Event()
                    current_event.record(self.stream)
            
            prev_event = current_event
            prev_buffer = current_buffer
            prev_neighbor_ids = neighbor_ids
            prev_count = len(neighbor_ids)
        
        # å¤„ç†æœ€åä¸€æ‰¹
        if prev_event is not None:
            prev_event.synchronize()
            pred_np = prev_buffer[:prev_count].numpy() * 255
            
            for i in range(len(prev_neighbor_ids)):
                idx = prev_neighbor_ids[i]
                img = (
                    np.array(pred_np[i]).astype(np.uint8)
                    * binary_masks_chunk[idx]
                    + frames_np_chunk[idx] * (1 - binary_masks_chunk[idx])
                )
                if comp_frames_chunk[idx] is None:
                    comp_frames_chunk[idx] = img
                else:
                    comp_frames_chunk[idx] = (
                        comp_frames_chunk[idx].astype(np.float32) * 0.5
                        + img.astype(np.float32) * 0.5
                    )
        
        torch.cuda.synchronize()
        elapsed = perf_counter() - start_time
        raise StopAfterOneChunk(elapsed, comp_frames_chunk)


# =============================================================================
# æµ‹è¯•æ¡†æ¶
# =============================================================================

@dataclass
class Result:
    name: str
    time: float
    gpu_mb: float
    success: bool = True


def clear_gpu():
    gc.collect()
    torch.cuda.empty_cache()
    torch.cuda.synchronize()


def benchmark(name: str, sora_wm: SoraWM, cleaner, input_path: Path, output_path: Path) -> Result:
    """è¿è¡Œå•ä¸ªåŸºå‡†æµ‹è¯•"""
    console.print(f"[cyan]â–¶ æµ‹è¯•: {name}[/cyan]")
    
    clear_gpu()
    
    sora_wm.cleaner = cleaner
    
    torch.cuda.reset_peak_memory_stats()
    torch.cuda.synchronize()
    
    try:
        sora_wm.run(input_path, output_path)
        # æ­£å¸¸å®Œæˆä¸åº”è¯¥åˆ°è¿™é‡Œ
        console.print(f"[yellow]  âš  æœªèƒ½æ•è·åˆ°è®¡æ—¶[/yellow]")
        return Result(name, 0, 0, False)
        
    except StopAfterOneChunk as e:
        # æˆåŠŸæ•è·è®¡æ—¶
        gpu_mb = torch.cuda.max_memory_allocated() / (1024 ** 2)
        console.print(f"[green]  âœ“ {e.elapsed_time:.2f}s, GPU: {gpu_mb:.0f}MB[/green]")
        return Result(name, e.elapsed_time, gpu_mb, True)
        
    except Exception as e:
        console.print(f"[red]  âœ— é”™è¯¯: {e}[/red]")
        import traceback
        traceback.print_exc()
        return Result(name, 0, 0, False)


def print_results(results: List[Result], baseline_time: float):
    """æ‰“å°ç»“æœè¡¨æ ¼"""
    
    table = Table(
        title="ğŸš€ æ€§èƒ½åŸºå‡†æµ‹è¯•ç»“æœ (å• Chunk)",
        box=box.ROUNDED,
        header_style="bold magenta",
    )
    
    table.add_column("æ–¹æ¡ˆ", style="cyan", width=38)
    table.add_column("è€—æ—¶", justify="right", width=12)
    table.add_column("åŠ é€Ÿæ¯”", justify="right", width=10)
    table.add_column("èŠ‚çœ", justify="right", width=12)
    table.add_column("GPUå³°å€¼", justify="right", width=12)
    table.add_column("çŠ¶æ€", justify="center", width=6)
    
    for r in results:
        if not r.success:
            table.add_row(r.name, "-", "-", "-", "-", "[red]âœ—[/red]")
            continue
        
        if baseline_time > 0:
            speedup = baseline_time / r.time if r.time > 0 else 0
            saved = baseline_time - r.time
        else:
            speedup = 1.0
            saved = 0
        
        # åŠ é€Ÿæ¯”é¢œè‰²
        if speedup >= 1.5:
            sp_str = f"[bold green]{speedup:.2f}x[/bold green]"
        elif speedup >= 1.2:
            sp_str = f"[green]{speedup:.2f}x[/green]"
        elif speedup >= 1.0:
            sp_str = f"[yellow]{speedup:.2f}x[/yellow]"
        else:
            sp_str = f"[red]{speedup:.2f}x[/red]"
        
        # èŠ‚çœæ—¶é—´é¢œè‰²
        if saved > 0:
            saved_str = f"[green]-{saved:.2f}s[/green]"
        elif saved < 0:
            saved_str = f"[red]+{abs(saved):.2f}s[/red]"
        else:
            saved_str = "-"
        
        table.add_row(
            r.name,
            f"{r.time:.2f}s",
            sp_str,
            saved_str,
            f"{r.gpu_mb:.0f}MB",
            "[green]âœ“[/green]",
        )
    
    console.print()
    console.print(table)
    
    # æ‰¾æœ€ä½³
    successful = [r for r in results if r.success]
    if len(successful) >= 2 and baseline_time > 0:
        best = min(successful, key=lambda x: x.time)
        improvement = (baseline_time - best.time) / baseline_time * 100
        
        console.print()
        console.print(Panel(
            f"ğŸ† æœ€ä½³æ–¹æ¡ˆ: [bold green]{best.name}[/bold green]\n"
            f"â±ï¸  è€—æ—¶: [cyan]{best.time:.2f}s[/cyan]\n"
            f"ğŸ“ˆ æå‡: [bold yellow]{improvement:.1f}%[/bold yellow]\n"
            f"ğŸ’¾ èŠ‚çœ: [green]{baseline_time - best.time:.2f}s[/green]",
            title="ğŸ“Š ç»“è®º",
            border_style="green",
        ))


# =============================================================================
# ä¸»ç¨‹åº
# =============================================================================

if __name__ == "__main__":
    input_video_path = Path("resources/dog_vs_sam.mp4")
    output_dir = Path("outputs/benchmark")
    output_dir.mkdir(parents=True, exist_ok=True)
    
    console.print(Panel.fit(
        f"[bold]E2FGVI HD Cleaner æ€§èƒ½æµ‹è¯•[/bold]\n"
        f"è¾“å…¥: {input_video_path}\n"
        f"[dim]ä»…æµ‹è¯•å•ä¸ª chunk çš„å¤„ç†æ—¶é—´[/dim]",
        title="ğŸ§ª Benchmark",
        border_style="cyan",
    ))
    
    # åˆå§‹åŒ–
    console.print("\n[yellow]åˆå§‹åŒ– SoraWM...[/yellow]")
    sora_wm = SoraWM(cleaner_type=CleanerType.E2FGVI_HQ)
    
    results = []
    
    # ========== æµ‹è¯• 1: åŸå§‹ç‰ˆæœ¬ (åŸºå‡†) ==========
    r = benchmark(
        "â‘  Original (Baseline)",
        sora_wm,
        BaselineBenchmarkCleaner(),
        input_video_path,
        output_dir / "baseline.mp4",
    )
    results.append(r)
    baseline_time = r.time if r.success else 0
    
    # ========== æµ‹è¯• 2: åŒç¼“å†²ä¼˜åŒ– ==========
    r = benchmark(
        "â‘¡ Optimized (åŒç¼“å†² + CUDA Streams)",
        sora_wm,
        OptimizedBenchmarkCleaner(),
        input_video_path,
        output_dir / "optimized.mp4",
    )
    results.append(r)
    
    # ========== æµ‹è¯• 3: çº¿ç¨‹æ± ä¼˜åŒ– ==========
    r = benchmark(
        "â‘¢ ThreadPool (å¤šçº¿ç¨‹ + Streams)",
        sora_wm,
        ThreadPoolBenchmarkCleaner(num_cpu_workers=4),
        input_video_path,
        output_dir / "threadpool.mp4",
    )
    results.append(r)
    
    # ========== æµ‹è¯• 4: Pinned Memory ä¼˜åŒ– ==========
    r = benchmark(
        "â‘£ PinnedMemory (é”é¡µå†…å­˜ + å¼‚æ­¥ä¼ è¾“)",
        sora_wm,
        PinnedMemoryBenchmarkCleaner(max_neighbors=20),
        input_video_path,
        output_dir / "pinned.mp4",
    )
    results.append(r)
    
    # è¾“å‡ºç»“æœ
    print_results(results, baseline_time)